# -*- coding: utf-8 -*-
"""Copy of Project 4 - battery electrode design

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KZQQJkVUmx53xRMkICEzWyc9zrutTOl5

Battery Electrode Design

The objective of this project is to design an electrode that has high adhesion and rate capability to meet high energy requirements.

The inputs are:


1.   Surfactant type (0 or 1)
2.   Carbon 1 wt%

1.   Carbon 2 wt%
2.   Binder wt%
1.   Surfactant wt%

The outputs are:


1.   Adhesion (lbf/in)
2.   Rate capability (mAh/g)
"""



#task 1 - import & normalize the data
#1.1 - import the data
#for PCA analysis, excluded surfactant type

import math
import numpy as np
from copy import copy, deepcopy
import pandas as pd

#input data: [CB1 (Wt%), CB2 (Wt%), Binder (Wt%), Surfactant (Wt%)]
xdata = [[1.25,0.42,1.33,0.1002],
  [0.8525,0.6975,1.45,0.1002],
  [1,0.4,1.66,0.063],
  [1.5,0.5,1,0.12],
  [1,0.33,1.66,0.0798],
  [1.25,0.42,1.33,0.008016],
  [1.5,0.5,1,0.1],
  [1.8,0.2,1,0.1],
  [1.25,0.42,1.33,0.0835],
  [1,0.5,1,0.0072],
  [1.516,0.493,1.250,0.064],
  [0.835,0.552,1.222,0.053],
  [1.088,0.188,1.297,0.011],
  [1.096,0.535,1.324,0.059],
  [0.981,0.228,1.093,0.091],
  [0.941,0.556,1.022,0.020],
  [1.044,0.051,1.534,0.068],
  [0.938,0.049,1.479,0.119],
  [1.710,0.484,1.095,0.118],
  [1.450,0.178,1.118,0.095],
  [1.061,0.352,1.503,0.108],
  [1.209,0.562,1.277,0.110],
  [1.255,0.640,1.043,0.083],
  [1.010,0.111,1.474,0.020],
  [0.935,0.142,1.441,0.111],
  [0.907,0.625,1.396,0.109],
  [1.232,0.066,1.522,0.082],
  [0.888,0.034,1.308,0.077],
  [1.731,0.395,1.376,0.027],
  [1.689,0.085,1.215,0.105],
  [1.533,0.573,1.192,0.117],
  [1.243,0.219,1.348,0.116],
  [1.122,0.344,1.034,0.081],
  [0.908,0.323,1.641,0.111],
  [1.511,0.606,1.124,0.072],
  [1.743,0.666,1.089,0.035],
  [1.284,0.506,1.089,0.110],
  [1.319,0.171,1.664,0.078],
  [1.729,0.203,1.004,0.084],
  [1.548,0.474,1.122,0.035]]


#output data: [adhesion (lbf/in), 2C dQ (mAh/g)]
ydata = [[1.698, 181],
 [1.881,177],
 [1.924,183],
 [0.513,181.3],
 [2.502,172],
 [1.278,182],
 [0.99,182],
 [1.395,184.5],
 [1.929,173],
 [0.348,187.5],
 [0.012,217.921],
 [1.769,145.788],
 [2.008,144.828],
 [0.812,175.122],
 [3.978,103.032],
 [1.942,137.754],
 [2.654,130.337],
 [5.460,113.349],
 [0.840,217.995],
 [1.566,149.529],
 [3.494,164.392],
 [1.743,182.408],
 [0.000,182.029],
 [3.335,138.018],
 [4.215,122.447],
 [2.737,170.113],
 [3.171,155.647],
 [5.135,90.649],
 [0.000,245.988],
 [2.089,184.095],
 [0.669,213.180],
 [2.808,151.043],
 [1.902,128.582],
 [4.289,155.332],
 [0.000,204.092],
 [0.000,246.724],
 [2.773,172.825],
 [2.527,183.636],
 [1.691,178.438],
 [0.000,215.416]]


xarray= np.array(xdata)
yarray= np.array(ydata)

'''print("length of xarray", len(xarray))
print("length of yarray", len(yarray))'''

#1.2 normalize the data
#compute mean and std
xdata_mean = np.mean(xdata, axis = 0)
xdata_std = np.std(xdata, axis = 0)

#represent mean and std values as variables
Cb1mean = xdata_mean[0] #carbon black 1
Cb2mean = xdata_mean[1] #carbon black 2
Bmean = xdata_mean[2] #binder
Smean = xdata_mean[3] #surfactant

Cb1std = xdata_std[0]
Cb2std = xdata_std[1]
Bstd = xdata_std[2]
Sstd = xdata_std[3]

#print arrays to check
print("mean", xdata_mean)
print("std", xdata_std)
print("Cb1mean",Cb1mean)
print("Cb2mean",Cb2mean)
print("Bmean", Bmean)
print("Cb1std", Cb1std)
print("Cb2std", Cb2std)
print("Bstd", Bstd)
print("Sstd", Sstd)

#standardize the xdata set
#subtract mean and divide by std

xdatanorm=[]
xdatanorm= deepcopy(xdata)

for i in range(len(xdata)):
    xdatanorm[i][0] = (xdata[i][0] - Cb1mean)/(Cb1std)
    xdatanorm[i][1] = (xdata[i][1] - Cb2mean)/(Cb2std)
    xdatanorm[i][2] = (xdata[i][2] - Bmean)/(Bstd)
    xdatanorm[i][3] = (xdata[i][3] - Smean)/(Sstd)

#store normalized xdata in array
xnormarray = np.array(xdatanorm)

#print normalized data to check
print("normalized xdata")
print(np.round(xnormarray, 4))
print()

# Commented out IPython magic to ensure Python compatibility.
#task 2A - perform PCA to identify most important features (&transform the data if necessary).
#2.1 - PCA
#the purpose of PCA is to check covariant features and reduce the dimensionality of data if necessary

import numpy as np
from numpy import linalg as LA
import tensorflow as tf

X = xnormarray #xnormarray is normalized (-mean/std) xdata
C = np.cov(X.T)  #compute covariance matrix
print('Format of Covariance matrix (symmetric matrix):')
print('[covar(x1, x2, x3, x4), var(x2), var(x3), var(x4)]')
print('[var(x1), covar(x1, x2, x3, x4), var(x3), var(x4)]')
print('[var(x1), var(x2), covar(x1, x2, x3, x4), var(x4)]')
print('[var(x1), var(x2), var(x3), covar(x1, x2, x3, x4)]')
print()
print('Covariance Matrix')
print(np.round(C,3))
print()



w, v = LA.eig(C)  #get the eigenvalues w and the eigenvectors v
print ("Eigenvalues:")
print(np.round(w,9))
print()
print("Eigenvectors:")
print(np.round(v,9))
print()


#task 2.2
#PCA plot
# %matplotlib notebook
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

fig = plt.figure()

ax = fig.gca(projection='3d')
fig.set_size_inches(5,7)

X = xnormarray #normalized xdata
x=X[:,0]
y=X[:,1]
z=X[:,2]

ax = fig.add_subplot(111, projection='3d')
ax.scatter(x,y,z, c='red', s=60)

plt.title('Scatterplot of standardized input features')
ax.set_xlabel('Cb1%')
ax.set_ylabel('Cb2%')
ax.set_zlabel('binder%');

#rotate the view
ax.view_init(30, 225)
plt.show()

#task 2B - split the data into 80-20 training & validation set
#using data with surfactant type (0 or 1)

import math
import numpy as np
from copy import copy, deepcopy
import pandas as pd

#input data: [surfactant type, CB1 (Wt%), CB2 (Wt%), Binder (Wt%), Surfactant (Wt%)]
xdata = [[0,1.25,0.42,1.33,0.1002],
  [0, 0.8525,0.6975,1.45,0.1002],
  [0,1,0.4,1.66,0.063],
  [0,1.5,0.5,1,0.12],
  [0,1,0.33,1.66,0.0798],
  [1,1.25,0.42,1.33,0.008016],
  [1,1.5,0.5,1,0.1],
  [1,1.8,0.2,1,0.1],
  [1,1.25,0.42,1.33,0.0835],
  [1,1,0.5,1,0.0072],
  [0,1.516,0.493,1.250,0.064],
  [0,0.835,0.552,1.222,0.053],
  [0,1.088,0.188,1.297,0.011],
  [0,1.096,0.535,1.324,0.059],
  [1,0.981,0.228,1.093,0.091],
  [1,0.941,0.556,1.022,0.020],
  [1,1.044,0.051,1.534,0.068],
  [0,0.938,0.049,1.479,0.119],
  [0,1.710,0.484,1.095,0.118],
  [0,1.450,0.178,1.118,0.095],
  [0,1.061,0.352,1.503,0.108],
  [0,1.209,0.562,1.277,0.110],
  [0,1.255,0.640,1.043,0.083],
  [1,1.010,0.111,1.474,0.020],
  [0,0.935,0.142,1.441,0.111],
  [1,0.907,0.625,1.396,0.109],
  [0,1.232,0.066,1.522,0.082],
  [1,0.888,0.034,1.308,0.077],
  [0,1.731,0.395,1.376,0.027],
  [0,1.689,0.085,1.215,0.105],
  [0,1.533,0.573,1.192,0.117],
  [1,1.243,0.219,1.348,0.116],
  [0,1.122,0.344,1.034,0.081],
  [1,0.908,0.323,1.641,0.111],
  [1,1.511,0.606,1.124,0.072],
  [1,1.743,0.666,1.089,0.035],
  [1,1.284,0.506,1.089,0.110],
  [1,1.319,0.171,1.664,0.078],
  [1,1.729,0.203,1.004,0.084],
  [0,1.548,0.474,1.122,0.035]]


#output data: [adhesion (lbf/in), 2C dQ (mAh/g)]
ydata = [[1.698, 181],
 [1.881,177],
 [1.924,183],
 [0.513,181.3],
 [2.502,172],
 [1.278,182],
 [0.99,182],
 [1.395,184.5],
 [1.929,173],
 [0.348,187.5],
 [0.012,217.921],
 [1.769,145.788],
 [2.008,144.828],
 [0.812,175.122],
 [3.978,103.032],
 [1.942,137.754],
 [2.654,130.337],
 [5.460,113.349],
 [0.840,217.995],
 [1.566,149.529],
 [3.494,164.392],
 [1.743,182.408],
 [0.000,182.029],
 [3.335,138.018],
 [4.215,122.447],
 [2.737,170.113],
 [3.171,155.647],
 [5.135,90.649],
 [0.000,245.988],
 [2.089,184.095],
 [0.669,213.180],
 [2.808,151.043],
 [1.902,128.582],
 [4.289,155.332],
 [0.000,204.092],
 [0.000,246.724],
 [2.773,172.825],
 [2.527,183.636],
 [1.691,178.438],
 [0.000,215.416]]


xarray= np.array(xdata)
yarray= np.array(ydata)

# define median values of input variables
xdata_med = np.median(xdata, axis = 0)
print("xdata median", xdata_med)

#store input median values in variables
Cb1med = xdata_med[1] #Cb1
Cb2med = xdata_med[2] #Cb2
Bmed = xdata_med[3] #Binder
Smed = xdata_med[4] #surfactant
'''it is uncessary to normalize the surfactant type (index 0)'''

# define median values of output variables
ydata_med = np.median(ydata, axis = 0)
print("ydata median", ydata_med)

#store output median values in variables
Amed = ydata_med[0] #adhesion (lbf/in)
Qmed = ydata_med[1] #rate capability Q (mAh/g)

#define normalized input and output data
#standardize the xdata and ydata set
#divide by median

xdatatrain=[] #xdatatrain is xdata divided by median
xdatatrain= deepcopy(xdata)
ydatatrain=[] #ydatatrain is ydata divided by median
ydatatrain= deepcopy(ydata)

for i in range(len(xdata)):
    xdatatrain[i][1] = xdata[i][1]/Cb1med
    xdatatrain[i][2] = xdata[i][2]/Cb2med
    xdatatrain[i][3] = xdata[i][3]/Bmed
    xdatatrain[i][4] = xdata[i][4]/Smed

for j in range(len(ydata)):
    ydatatrain[j][0] = ydata[j][0]/Amed
    ydatatrain[j][1] = ydata[j][1]/Qmed

#store normalized data in arrays
xdataarr = np.array(xdatatrain)
ydataarr = np.array(ydatatrain)

print("normalized xdata array")
print(np.round(xdataarr, 3))
print("normalized ydata array")
print(np.round(ydataarr, 3))

#task 2B
#split xdataarr and ydataarr into 4/5 training and 1/5 validation

import numpy as np
from sklearn.model_selection import train_test_split
import pandas as pd

x_train, x_test, y_train, y_test = train_test_split(xdataarr,ydataarr,
test_size=0.2, random_state=1234)

#print to confirm
print("xtrain", pd.DataFrame(x_train))
print("\nlength xtrain", len(x_train))

print("\nxtest", pd.DataFrame(x_test))
print("length xtest", len(x_test))

print("\nytrain", pd.DataFrame(y_train))
print("length ytrain", len(y_train))

print("\nytest", pd.DataFrame(y_test))
print("length ytest", len(y_test))

#task 3 - set up the neural network
# define neural network model
#w/dropout layers
#In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob.

#As seen below, we have created 4 dense layers.
#A dense layer is a layer in neural network thatâ€™s fully connected.
#In other words, all the neurons in one layer are connected to all other neurons in the next layer.
#In the first layer, we need to provide the input shape, which is 5 in our case.
#The activation function we have chosen is elu, which stands for exponential linear unit. .

#import useful packages
import keras
import pandas as pd
from keras.models import Sequential
import numpy as np
import keras.backend as kb
import tensorflow as tf
from keras import backend as K
#initialize weights with values between -0.2 and 0.5
initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=0.5)

model = keras.Sequential([
    keras.layers.Dense(10, activation=K.elu, input_shape=[5],  kernel_initializer=initializer),
    keras.layers.Dense(14, activation=K.elu,  kernel_initializer=initializer),
    keras.layers.Dense(18, activation=K.elu, kernel_initializer=initializer),
    keras.layers.Dropout(0.1),
    keras.layers.Dense(16, activation=K.elu, kernel_initializer=initializer),
    keras.layers.Dropout(0.1),
    keras.layers.Dense(2,  kernel_initializer=initializer)
  ])

#task3
#Weâ€™re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation.
#Itâ€™s one of the most popular gradient descent optimization algorithms for deep learning networks.
#RMSprop is an optimizer thatâ€™s reliable and fast.
#Weâ€™re compiling the mode using the model.compile function. The loss function used here
#is mean squared error. After the compilation of the model, weâ€™ll use the fit method with ~500 epochs.
#Number of epochs can be varied.

#from tf.keras import optimizers
rms = keras.optimizers.RMSprop(0.008) #variable learning rates 0.1-0.05-0.008-0.003-0.001
model.compile(loss='mean_absolute_error',optimizer=rms, metrics=['accuracy'])

#SUMMARIZE MODEL
'''To visualize the layers that created in the above step, use the summary function.
This will show some parameters (weights and biases) in each layer and also the total parameters in your model.'''
model.summary()

#task 3
#After the compilation of the model, weâ€™ll use the fit method with 1500 epochs.

#The fit method takes three parameters; namely, x, y, and number of epochs.
#During model training, if all the batches of data are seen by the model once,
#we say that one epoch has been completed.

# Add an early stopping callback
es = keras.callbacks.EarlyStopping(
    monitor='loss',
    mode='min',
    patience = 200,
    restore_best_weights = True,
    verbose=1)
# Add a checkpoint where loss is minimum, and save that model
mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss',
                     mode='min',  verbose=1, save_best_only=True)

historyData = model.fit(x_train,y_train,epochs=1500,callbacks=[es])

loss_hist = historyData.history['loss']
#The above line will return a dictionary, access it's info like this:
best_epoch = np.argmin(historyData.history['loss']) + 1
print ('best epoch = ', best_epoch)
print('smallest loss =', np.min(loss_hist))

model.save('./best_model')

# This line of code can be used to reconstruct the saved model.

recon_model = keras.models.load_model("best_model")

# the name of the model is now "recon_model". You can then use this model to do predictions for comparisons.
# See the previous project for code to do the comparisons.

#task 4
#log-log plot of actual vs predicted rate outputs
#data renormalized

import matplotlib.pyplot as plt
import numpy as np
from copy import deepcopy
import pandas as pd
from sklearn.metrics import r2_score, mean_absolute_error

#assemble measQ data
#measQ data is from y_train [i][1]
measQ = []
for i in range(len(y_train)):
    measQ.append(y_train[i][1]*Qmed) #access y_train rate Q data
'''print("measQ", pd.DataFrame(measQ))'''

#assemble predQ data
predQ =[]
xpred= []
xpred = deepcopy(x_train)
output = []

for i in range(len(x_train)): #access xdata
    xpred [i][0] = x_train[i][0]
    xpred [i][1] = x_train[i][1]
    xpred [i][2] = x_train[i][2]
    xpred [i][3] = x_train[i][3]
    xpred [i][4] = x_train[i][4]
    xpredarray = np.array(xpred) #convert to array data
    output = recon_model.predict(xpredarray) #use NN to predict, 2 outputs
'''print("output", pd.DataFrame(output))'''

for j in range(len(output)):
    predQ.append(output[j][1]*Qmed) #renormalize Q output at [j][1]
'''print("predQ",pd.DataFrame(predQ))'''


#plot the data
plt.scatter(predQ, measQ, s=25)
plt.plot(measQ, measQ, label = 'Perfect model prediction', color = 'black', linestyle= '--')
plt.xlabel('Predicted Rate (mAh/g)', fontsize = 16)
plt.ylabel('Measured Rate (mAh/g)', fontsize = 16)
plt.title("Rate Capability Training Prediction Log-Log Plot", fontsize = 18)
plt.legend(loc='upper left')
plt.loglog()
plt.show()

#calc correlation & MAE
print("R2")
print(np.round(r2_score(measQ, predQ),3))

print("mean abs error")
print(np.round(mean_absolute_error(measQ, predQ),3))

#task 5- compare the training with validation data
#log-log plot of actual vs predicted rate outputs
#data renormalized

import matplotlib.pyplot as plt
import numpy as np
from copy import deepcopy
import pandas as pd
from sklearn.metrics import r2_score, mean_absolute_error

#assemble measQ data
#measQ data is from y_test [i][1]
measQ1 = []
for i in range(len(y_test)):
    measQ1.append(y_test[i][1]*Qmed) #access y_test rate Q data
print("measQ", pd.DataFrame(measQ1))

#assemble predQ data
predQ1 =[]
xpred1= []
xpred1 = deepcopy(x_test)
output1 = []

for i in range(len(x_test)): #access x_test data
    xpred1 [i][0] = x_test[i][0]
    xpred1 [i][1] = x_test[i][1]
    xpred1 [i][2] = x_test[i][2]
    xpred1 [i][3] = x_test[i][3]
    xpred1 [i][4] = x_test[i][4]
    xpredarray1 = np.array(xpred1) #convert to array data
    output1 = recon_model.predict(xpredarray1) #use NN to predict, 2 outputs
print("output", pd.DataFrame(output1))

for j in range(len(output1)):
    predQ1.append(output1[j][1]*Qmed) #renormalize Q output at [j][1]
print("predQ",pd.DataFrame(predQ1))


#plot the data
plt.scatter(predQ1, measQ1, s=25)
plt.plot(measQ1, measQ1, label = 'Perfect model prediction', color = 'red', linestyle= '--')
plt.xlabel('Predicted Rate (mAh/g)', fontsize = 16)
plt.ylabel('Measured Rate (mAh/g)', fontsize = 16)
plt.title("Rate Capability Validation Log-Log Plot", fontsize = 18)
plt.legend(loc='upper left')
plt.loglog()
plt.show()

#calc correlation & mean absolute error
print("R2")
print(np.round(r2_score(measQ1, predQ1),3))

print("mean abs error")
print(np.round(mean_absolute_error(measQ1, predQ1),3))



#Task 6
# Use model to predict output with given range of inputs. Generate 3D surface plot.
#Given a constant surfactant type and Carbon wt%, let's develop a 3d surface plot to see how the adhesion varies with binder and surfactant wt%.

import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator
import numpy as np

#surfact type = 0
st = 0
#carbon 1 wt% = 1
c1 = 1
#carbon 2 wt% = 0.5
c2 = 0.5
#binder wt%
bind = np.linspace(1,2,50)
#surfactant wt%
surf = np.linspace(0.005,0.2,50)

#generate the data for the plot using the recon model and meshgrid
Z = np.zeros((len(bind),len(surf)))
for i in range(len(bind)):
    for j in range(len(surf)):
      #normalize data before modelling
        inpt = np.array([[st,c1/Cb1med, c2/Cb2med, bind[i]/Bmed,surf[j]/Smed]])
        outpt = recon_model.predict(inpt)
      #re-normalize the output
        Z[j][i] = outpt[0][0]*Amed
X,Y = np.meshgrid(bind,surf)

#create the 3D surface plot
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
surf = ax.plot_surface(X,Y,Z,cmap=cm.coolwarm,linewidth=0, antialiased=False)
plt.xlabel('Binder wt%')
plt.ylabel('Surfactant wt%')
ax.set_zlabel('Adhesion (lbf/in)')
plt.title('Surface Plot of Adhesion Predictions from Varying Binder and Surfactant wt%')
plt.show()

#Task 7
# Use model to predict best design for (1) max adhesion, (2) max rate, and (3) high adhesion + rate

#find inputs where max adhesion is acheived
adh = []
for i in range(len(ydata)):
  adh.append(ydata[i][0])
ind = adh.index(max(adh))
max_adh = xdata[ind] + [adh[ind]] + [rate[ind]] + ['Max Adhesion']
print('Inputs that maximize adhesion:')
xdata[ind]

#find inputs where max rate capability  is acheived
rate = []
for i in range(len(ydata)):
  rate.append(ydata[i][1])
ind = rate.index(max(rate))
max_rate = xdata[ind] + [adh[ind]] + [rate[ind]] + ['Max Rate Capability']
# print('Inputs that maximize rate capability:')
xdata[ind]

#plot the outputs of both variables
plt.scatter(adh,rate)
plt.title('Outputs of Adhesion vs Rate Capability')
plt.xlabel('Adhesion (lbf/in)')
plt.ylabel('Rate Capability (mAh/g)')
plt.show()

#Now try to maximize both outputs -> both outputs seperately are inversely correlated, so need to choose values in the middle around 2-3 lbf/in and 160-180 mAh/g.

# make a table to view the outputs easily

table=pd.DataFrame(ydata).rename(columns={0:'Adhesion',1:'Rate Capability'})
#normalize the table by the max values
table['Adhesion'] = table['Adhesion']/max(table['Adhesion'])
table['Rate Capability'] = table['Rate Capability']/max(table['Rate Capability'])
#filter the table to choose outputs that achieve atleast 50% of the max output
table = table[table['Adhesion']>0.50]
table = table[table['Rate Capability']>0.50]
table.sort_values('Rate Capability')

table = table.reset_index()
table

#indices of the top outputs
out_ind = table.index
best_inputs = []
for i in out_ind:
  best_inputs.append(xdata[i])
best_inputs

#turn the best inputs and outputs into a table

best = pd.DataFrame(best_inputs)
table.reset_index()
best['Adhesion (lbf/in)'] = table['Adhesion'] * max(adh)
best['Rate Capability (mAh/g)'] = table['Rate Capability'] * max(rate)
best['Design Type'] = ['Optimal' for i in range(len(best))]
best=best.rename(columns={0:'Surfactant Type',1:'Carbon 1 wt%',2:'Carbon 2 wt%',3:'Binder wt%',4:'Surfactant wt%'})
#return the table with the electrode design that produces both the optimal and adhesion and rate capability
best

#make final table including design parameters for every scenario
maxes = pd.DataFrame([max_adh,max_rate])
maxes=maxes.rename(columns={0:'Surfactant Type',1:'Carbon 1 wt%',2:'Carbon 2 wt%',3:'Binder wt%',4:'Surfactant wt%',5:'Adhesion (lbf/in)',6:'Rate Capability (mAh/g)',7:'Design Type'})
best=maxes.append(best).reset_index().drop('index',axis=1)

#output table for the input and output for the best battery electrode designs
best

#Task 8 - Battery Electrode Testing
#predict the outputs for 3 different scenarios

#function to normalize easily
def norm(x):
  for i in range(len(x)):
    x[i][1] = x[i][1]/Cb1med
    x[i][2] = x[i][2]/Cb2med
    x[i][3] = x[i][3]/Bmed
    x[i][4] = x[i][4]/Smed
  return np.array(x)

#scenario 1
s1 = [[0,1.3,0.5,1.2,0.1],[1,1.3,0.5,1.2,0.1]]
s1=norm(s1)
np.multiply(recon_model.predict(s1),[Amed,Qmed])

#scenario 2
s2 = [[1,0.75,0.75,1.5,0.1]]
s2=norm(s2)
np.multiply(recon_model.predict(s2),[Amed,Qmed])

#scenario 3
s3 = [[1,0.75,0.75,1.4,0.2]]
s3=norm(s3)
np.multiply(recon_model.predict(s3),[Amed,Qmed])

